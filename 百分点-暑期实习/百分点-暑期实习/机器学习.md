# 机器学习

## 机器学习定义

人工智能一小类，编写程序求解问题。使用领域多。

基础：大量的数据

优势：不知道如何编写程序时，可使用机器学习。自定制软件、学习人类学习。

- 定义1

	没有明确设置，使计算机具有学习能力的研究领域

- 定义2

	计算机程序从经验E中学习，解决某一任务T进行某一性能度量P，通过P测定在T的表现因E提高

机器学习分类：

最热门

- 监督学习（Supervised learning）

- 无监督学习（Unsupervised learning）

## 监督学习

![]("/img/20230718103215.png")

房价预测：使用函数\直线 拟合数据 进行预测

监督学习定义：给一个数据集，包括“right answer”

目的：给出更真实的预测

**称之为回归问题(Regression)：想要预测连续的数值输出**
Regression:设法预测连续值的属性

另外的例子

![]("/img/20230718103857.png")

给定大小，判断肿瘤癌良性（0），恶性（1）

**称之为分类问题(Classification problem):**期望离散的输出值，比如说0或1，有时是多种。


## 无监督学习

![]("/img/20230718104839.png")

给定数据集，让机器自己推断，在这个例子中，机器可能会将所有数据分为两簇。

![]("/img/20230718105246.png")

谷歌搜索成千上万的新闻，自动将其分簇，显示在一起。

都称之为聚类算法(clustering)


## 模型

![]("/img/20230718110926.png")

房价预测。

![]("/img/20230718111303.png")

寻找合适的h(hypothesis)，使x->y

![]("/img/20230718111411.png")

最终拟合线性函数

![]("/img/20230718112520.png")

这种最终拟合为线性函数的回归问题称之为线性回归/单变量线性回归（因为单变量）

- 代价函数

	h(x)=kx+b
	在给定数据集上，选择最好的k与b使得 h(x)接近y对于训练集的样板(x,y)
	
	即尽可能减少
	![]("/img/20230718140515.png")
	
	并且由于训练集不一样，所以要*1/2m (m:样本数量)
	
	按照惯例，选出代价函数（cost function） J(k,b)使最小。本题中可使用上述平方误差代价函数为代价函数。
	
	平方误差代价函数对大多数问题，特别是回归问题都是一个合理、常规的选择。
	
	![]("/img/20230718142021.png")
	
	设函数为y=kx
	
	选择合适的值，获取最小的J(k1)最小值
	
	![]("/img/20230718142254.png")
	
	可以看到，当函数拥有两个参数k,b时，那么J就变为碗状曲面。
	
	还有一种表示方法等高线图 contour plot/figure
	
	![](/img/20230718142544.png)
	
	目标：编写软件，自动选取最好的k,b使J(k,b)最小


- 梯度下降(gradient descent)

	使用梯度下降计算最小的J

	![](/img/20230718142544.png)

	站在山顶，如果我想下山最快，那么往那边走？不断收敛，最终达到最小值。

	每次选择梯度下降最大的。不同的起点会对应不同局部最优解

	![](/img/20230718143953.png)

	同时更新k,b(thelta_0,0_1)

	k=k-某某 b=b-某某
	
	![](/img/20230718145638.png)



	- 名词解释
	
		a:=b  将b赋值给a
	
		a=b 判断a=b
	
		α：学习率，即“迈步下山”中的步子，α越大，则每次查看梯度的度量长度不同，从而可能会影响最优解的时间和结果。
	
		比如说，当步长过小时，算法收敛速度缓慢，需要更多的迭代次数才能找到最优解，也更容易陷入局部最优解以及鞍点，难以找到最优解。步长过长，可能略过最优解。

		![](/img/20230718150825.png)
	
	- 导数项

		![](/img/20230718150323.png)
		如图所示，首先在某一点k1上，求出偏导(斜率),新的k1=k1-正数，k1向左移，接近最小值。
		 
		同理，若斜率是负的，则k1增加，接近最小值
		
		
		当在局部最低点时，斜率=0，则根据公式，则不再下降。
		
		此外，在接近最低点时，可以发现斜率越来越接近0，所以每一步移动幅度越来越小，所以不需要一直减少α

	使用梯度下降最小化J
	![](/img/20230718151356.png) 

	计算导数：

	- 对于k(0_0):

		相当于z*(x+a)^2的求导（其中z=前面一大堆，x=km，a=后面那两个），得出导数=z*(2x+2a)

	- 对于b(0_1):

		相当于求z*(q*x+b)^2的导数，得出=2q*z*(q*x+b)
	![](/img/20230718152045.png)

	不断进行计算，直到都最小值
	
	![](/img/20230718152935.png)

	也可叫做Batch Gradient Descent Batch梯度下降法

## 线性代数知识

矩阵(Matrix)：数字组成的矩形写在方括号。	
矩阵维度(dimension)：rows*columns （4 by 2）

![](/img/20230718153831.png)

A12=1行2列=191

向量Vector：column=1的矩阵（n*1 matrix）

![](/img/20230718154210.png)

矩阵加法(addition)：逐个相加，维度不同不能加

标量（scalar）:数字或实数

标量*或÷矩阵= 逐个相乘/除

矩阵 * 向量：矩阵每行 * 向量每列

![]("/img/20230719101111.png")


实际应用：使用hypothesis计算房价

![](/img/20230719102141.png)

计算机中，使用矩阵计算效率更高。


矩阵a * 矩阵b=a*b每一列
![](/img/20230719102638.png)

特征：标量乘法可交换，矩阵不可，会改变结果。(associative)

矩阵乘法可以结合。(commutative)


## 多维特征

多个特征,也可叫做多元线性回归(multiple linear regression)

![](/img/20230719114233.png)

上标：第几个样本
下标：第几个特征

表示也不同

![]("/img/20230719114533.png")

![](/img/20230719114820.png)

在Python中使用向量,numpy使用并行计算，所以速度快很多

![](/img/20230719115257.png)

多元变量的梯度下降

![](/img/20230719140819.png)

## 特征缩放(feature scaling)

设price=w1x1+w2x2+b,如果实际情况下x1贼大，且x2贼小，那么就会出现下图特征：

![](/img/20230719154340.png)

梯度图会很瘪，这也就导致了梯度下降时会使每一个收敛容易过大，最终导致迭代次数过多。

关键：使两个参数影响相近，让梯度下降更容易

就本题来说,进行特征分析，有几种方法

- 使x1=x1/2000,x2=x2/5
- 均值归一法

	让每个值在（-1，1）之间
	方法：x-平均/max-min

![](/img/20230719155409.png)

- Z-score

	x-平均/标准差

![](/img/20230719155821.png)

特征的大小，选择不同的方式进行scaling


## 判断梯度下降 正确

J大小以及迭代次数

![](/img/20230719160841.png)

自动收敛测试

##如何设置学习率

若梯度下降持续波动或提升，可能代码问题（符号写错了）或者学习率过大，此时使用较小的学习率。

但是如果学习率太小，则可能迭代次数过多。

可以一开始使用较低学习率，随后逐渐增大，选择尽可能低学习率
![]("/img/20230719162153.png")

## 特征工程

选择正确特征是使算法运行良好的关键步骤

## 多项式回归（Polynomial regression）

当趋势为曲线时，可使用
f(x)=w1x+w2x**2+b或根号


## 逻辑回归(logistic regression)

输出0~1之间，一般用于解决二分类问题。
![](/img/20230720094401.png)


使用函数
sigmoid function/logistic function
![](/img/20230720094502.png)

两方程最终
![](/img/20230720100144.png)

其中，f(x)相当于变换算法，把特征变换到另外一个域上保证模型可分（弹幕说的，不保证正确）

若g(z)>0.5 一般分类为1 否则为0

![](/img/20230720100418.png)

根据图像，可以看到若g(z)>0.5，z>0，否则z<0
得出w*x+b>0时，分类为1(x不一定为1次方，可以为各种各样函数)

## 决策边界

若w*x+b=0时，称为决策边界，因为一旦超过这个边界，g(z)就会大于0.5，那么就会被分类为1。

![](/img/20230720101030.png)


总结：逻辑回归=设定决策边界，使线性回归拟合的曲线的输出转化为概率值，从而得出分类决策


## 逻辑回归中的代价函数

若使用平方误差代价函数，则可能不是一个好选择，由于J(w,b)不同于线性回归，那么会出现J(w,b)成本函数不断起伏，最终导致有多个最小值

![](/img/20230720102325.png)

方法：选择不同的代价函数L，使J(w,b)为凹函数，可以使用梯度下降。

![](/img/20230720102835.png)

具体如下:通过衡量一个样本在整体样本的情况判断好坏。

![](/img/20230720102920.png)

由于输入总是为0到1（因为逻辑回归），那么f(x)为0~1。

那么进行分类讨论，

- 若y=1（正分类）

	根据图，那么横轴（在这张图中，是f(x)）越接近1，则损失函数越小，否则损失函数越大

	![](/img/20230720104128.png)
 
- 若y=0 (负分类)
	
	横轴越接近0，那么损失函数越小
	
	![](/img/20230720104322.png)

整合，那么cost函数J则会变为

![](/img/20230720104524.png)

loss函数L的更简单写法

![](/img/20230720104815.png)

从而，cost函数J就会变为
![](/img/20230720104948.png)

还有各种各样的成本函数比如说最大似然函数(maximum likelihood)

## 逻辑回归的梯度下降

专注于找到w与b，使结果最好

![](/img/20230720105512.png)

求导后，得出每次梯度下降大小

![](/img/20230720110218.png)

## 过拟合问题(overfitting)

generalization 泛化，即使没见过的数据也能预测的很好

线性回归的欠拟合以及过拟合问题/高偏差(high variance)

![](/img/20230720111923.png)

函数尽可能符合所有的训练样本

过拟合可能使函数最终使函数在训练集表现好，但是在实际上表现并不好

![](/img/20230720112452.png)

**解决过拟合**

- 选择更多的训练样本

- 减少特征的选择

	过多特征，样本少会使得过拟合
	特征选择feature selection

- 正则化

	鼓励学习算法降低参数W，防止某些特征过度影响
	
	![](/img/20230720113420.png)

## 正则化

惩罚所有的W使特征对最终结果的影响减小，从而防止过拟合

一般只惩罚w。

两个任务，通过最小化J，使损失函数最小，从而使得模型拟合于数据，同时降低w对模型的影响，防止过拟合问题。

![](/img/20230720114522.png)

不同的lambda:

若lambda=0，那么可能过度关注损失函数，从而产生过拟合。

若lambda过大，那么就会过度关注正则化函数，此时W接近0，函数接近b，产生欠拟合

- 线性回归中的正则方法
	![](/img/20230720140416.png)
	
	alpha和lambda都是很小的正数。在每此迭代时都使wj*接近1的正数,从而使新的wj变小

	![](/img/20230720141128.png)

- 逻辑回归中的正则方法

	![](/img/20230720141411.png)

附：成本函数cost：总体的偏差值

损失函数lost：单个样本的偏差值

## 神经网络

早期动机：模仿人大脑进行思考，接近于“深度学习”。语音识别->图像识别->NLP-> 。。。

人脑思考：其他神经元的脉冲作为输入，神经元进行计算，输出脉冲给下一个神经元。

一层中，不同的神经元输入不同特征，输出后再作为下一层新的输入。激活值

输入4数字，在神经网络中转换为3个数字，再将3个数字作为输入，转化为1个数字。

![](/img/20230721095123.png)

实际应用中，不需要定义隐藏层的输入

举例：人脸识别

![](/img/20230721100904.png)

第一层找线，第二层找身体部分，第三层找粗糙脸，输出层找到人脸

![](/img/20230721101241.png)

在这些时，神经网络自动找到每个层的输入/输出从大量数据中

具体工作流程：

![](/img/20230721101801.png)

a[i]上标表示第i层的值。

![](/img/20230721101910.png)

g也可视为activation function

## 向前传播算法

计算从左到右成为正向传播
![](/img/20230721110938.png)

四个方向称之为反向传播。

一般来说，一开始神经元多，后面少

具体流程：以神经元的逻辑回归算法为例子

## TensorFlow

![](/img/20230721111635.png)

数据表示：
在numpy中：
![](/img/20230721140258.png)

![](/img/20230721140926.png)

使用Sequential构建神经网络模型

![](/img/20230721141443.png)

随后使用compile,fit训练,predict(x_new)预测

![](/img/20230721141636.png)

Sequential的第二种用法：

![](/img/20230721141920.png)

自定义向前传播函数

![](/img/20230721150417.png)


## 训练神经网络

![](/img/20230724094105.png)

- 三步骤：

	①指定如何在给定的输入特征x和参数w b的情况下输出
	
	②选择loss损失函数和cost成本函数

	③选择算法，主要是梯度下降让w,b更接近与准确值

![](/img/20230724095244.png)

①选择模型

![](/img/20230724095515.png)

②选择cost_function和loss_function之间学习的损失函数称之为二元交叉熵(binary cross entropy)，可以指定其作为损失函数

![](/img/20230724100418.png)

③使用梯度下降更新w,b,J

![](/img/20230724100727.png)

## sigmoid函数的替代方案

不应该只是用sigmoid作为激活函数，因为可能对于有些情况时，输入为0或1无法准确测定。

比如说ReLU函数(rectified linear unit)，但是一般来说，没有使用任何激活函数，当z>0时，g(z)=z，或者可以说使用线性激活函数。

![](/img/20230724101807.png)

二分类问题：使用sigmoid

回归问题：使用不同的激活函数

比如：预测明天的股市情况，使用线性，因为可以得正值或者负值。预测房价：使用ReLU,因为房价不会变为负数。

选择激活函数以及输出

![](/img/20230724105119.png)

目前，ReLU是非常常用的，对比sigmoid来说，其计算更加快捷，效率更高，更重要的是，在图像中，只有一部分变平，而sigmoid有两个，这使得当函数有一部分比较“肥胖”(fit)，梯度下降比较慢

![](/img/20230724111734.png)

隐藏层一般使用relu激活函数，还有各种tan h， LeakyReLU，swish激活函数

## 为什么需要激活函数

如果都使用线性函数(激活函数为g(z)=z)，那么与普通的线性函数没有区别,一般使用ReLU

![](/img/20230724112056.png)

![](/img/20230724112302.png)

## 多分类问题

多分类依旧是分类，离散值多种结果。

![](/img/20230724112803.png)


- Softmax

逻辑回归的增强版，多类分类上下文的二进制分类算法

将逻辑回归转化为两个问题 a_1 a_2 (相加=1)

![](/img/20230724113253.png)

Softmax回归

![](/img/20230724113451.png)

y可能有n个值

所以此时

![](/img/20230724113807.png)

SoftMax的cost成本函数:交叉熵损失函数

-log(aN),若真实结果是aJ，那么cost = -lnaJ

L = -Σ(y * log(p))

![](/img/20230724114252.png)

新的架构：

![](/img/20230724140850.png)

不同于其它函数，Softmax激活函数在最终输出时，所得的值除了本身的变量，还会由其他多个变量组成：

![](/img/20230724141923.png)

使用Softmax函数

![](/img/20230724142405.png)

from logits= True的使用,将下图中的a直接代入到lossfunction中，使得精度更高

![](/img/20230724172426.png)

推广到Softmax:

![](/img/20230724173122.png)

故现在都不太使用softmax而是linear，可以看到最终预测的时候需要使用softmax进行分类，得到正确的输出结果

![](/img/20230724173202.png)

## 多输出

与多分类不同

![](/img/20230724173430.png)

![](/img/20230724173722.png)


## 高级优化方法

Adam方法，可以自动选择学习率Alpha

![](/img/20230724174424.png)

![](/img/20230724174537.png)

使用：

需要设定初始值

![](/img/20230724174611.png)

## 其他的网络层模型

目前使用神经元，但是也有不同层比如说卷积层，每一个神经元可以“看到”图像的一部分

优点：看到全局、需要更少的训练数据

![](/img/20230724175548.png)

卷积神经网络

EKG信号和心电图分类

诊断患者有没有心脏问题

![](/img/20230724180734.png)

##计算图

![](/img/20230724181045.png)